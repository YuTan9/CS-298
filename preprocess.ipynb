{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = 0.3\n",
    "MAX_TOKENS = 4096\n",
    "CHUNK_SIZE = 16 #  English sentence average sentence legth: 15~20 / Chinese sentence: 8~14 \n",
    "LATENT_SIZE = 512\n",
    "\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text as tf_text\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_pickle('merged.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f6b1a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "en_examples, zh_examples = np.array(df.en_news), np.array(df.zh_news.str.replace('ï¿½', ''))\n",
    "en_examples = np.full(shape = en_examples.shape, fill_value = \"[START] \") + en_examples + np.full(shape = en_examples.shape, fill_value = \" [END]\")\n",
    "zh_examples = np.full(shape = zh_examples.shape, fill_value = \"[START] \") + zh_examples + np.full(shape = zh_examples.shape, fill_value = \" [END]\")\n",
    "np.random.seed(42)\n",
    "isTrain = np.random.rand(df.shape[0]) > VALIDATION_SIZE\n",
    "en_train = en_examples[isTrain]\n",
    "zh_train = zh_examples[isTrain]\n",
    "en_valid = en_examples[~isTrain]\n",
    "zh_valid = zh_examples[~isTrain]\n",
    "examples = {}\n",
    "examples['train'] = tf.data.Dataset.from_tensor_slices((zh_train, en_train))\n",
    "examples['validation'] = tf.data.Dataset.from_tensor_slices((zh_valid, en_valid))\n",
    "\n",
    "train_examples, val_examples = examples['train'], examples['validation']\n",
    "\n",
    "import spacy\n",
    "import pickle\n",
    "from scipy.spatial import KDTree\n",
    "from tqdm.notebook import tqdm\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self, lang):\n",
    "        if(lang == 'zh'):\n",
    "            self.tokenizer = spacy.load(\"zh_core_web_sm\")\n",
    "            self.tokenizer.tokenizer.pkuseg_update_user_dict([\"[START]\", \"[END]\"])\n",
    "            self.lang = 'zh'\n",
    "            self.fasttext_vectorizer = fasttext.load_model('cc.zh.300.bin')\n",
    "        elif(lang == 'en'):\n",
    "            self.tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "            self.tokenizer.tokenizer.add_special_case(\"[START]\", [{spacy.attrs.ORTH: \"[START]\"}])\n",
    "            self.tokenizer.tokenizer.add_special_case(\"[END]\", [{spacy.attrs.ORTH: \"[END]\"}])\n",
    "            self.lang = 'en'\n",
    "            self.fasttext_vectorizer = fasttext.load_model('cc.en.300.bin')\n",
    "        else:\n",
    "            self.tokenizer = None\n",
    "        self.vec_to_text = {}\n",
    "        self.tree = None\n",
    "    \n",
    "    def train(self, docs):\n",
    "        for doc in tqdm(docs):\n",
    "            for token in self.tokenizer(doc):\n",
    "                if(self.lang == 'en'):\n",
    "                    text = token.text.lower()\n",
    "                else:\n",
    "                    text = token.text\n",
    "                if(token.is_digit):\n",
    "                    for digit in [*token.text]:\n",
    "                        if(tuple(self.fasttext_vectorizer.get_word_vector(digit)) not in self.vec_to_text):\n",
    "                          self.vec_to_text[tuple(self.fasttext_vectorizer.get_word_vector(digit))] = digit\n",
    "                else:\n",
    "                    if(tuple(self.fasttext_vectorizer.get_word_vector(text)) not in self.vec_to_text):\n",
    "                        self.vec_to_text[tuple(self.fasttext_vectorizer.get_word_vector(text))] = text\n",
    "                \n",
    "        print()\n",
    "        print(f'Train summary:')\n",
    "        print(f'\\t{len(self.vec_to_text)} {self.lang} words learned')\n",
    "        print('Building KDTree...')\n",
    "        self.tree = KDTree(list(self.vec_to_text.keys()))\n",
    "        print('KDTree built')\n",
    "    def tokenize(self, sentence):\n",
    "        for token in self.tokenizer(sentence):\n",
    "            print(token.text, end='\\t')\n",
    "\n",
    "\n",
    "\n",
    "en_tok_obj = Tokenizer('en')\n",
    "zh_tok_obj = Tokenizer('zh')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c911d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f11754",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tok_obj.train(en_examples)\n",
    "with open('en_vec_to_text.300-split_digits.pkl', 'wb') as f:\n",
    "    pickle.dump(en_tok_obj.vec_to_text, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072683a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_tok_obj.train(zh_examples)\n",
    "with open('zh_vec_to_text.300-split_digits.pkl', 'wb') as f:\n",
    "    pickle.dump(zh_tok_obj.vec_to_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4573612",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('zh_vec_to_text.300-split_digits.pkl', 'rb') as f:\n",
    "    zh_tok_obj.vec_to_text = pickle.load(f)\n",
    "zh_tok_obj.tree = KDTree(list(zh_tok_obj.vec_to_text.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4bb48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('en_vec_to_text.300-split_digits.pkl', 'rb') as f:\n",
    "    en_tok_obj.vec_to_text = pickle.load(f)\n",
    "en_tok_obj.tree = KDTree(list(en_tok_obj.vec_to_text.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ef0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_vectorize(docs):\n",
    "    forReturn = []\n",
    "    for doc in docs:\n",
    "        arr = []\n",
    "        for token in en_tok_obj.tokenizer(doc):\n",
    "            if(token.is_digit):\n",
    "                for digit in [*token.text]:\n",
    "                    arr.append(en_tok_obj.fasttext_vectorizer.get_word_vector(digit))\n",
    "            else:\n",
    "                arr.append(en_tok_obj.fasttext_vectorizer.get_word_vector(token.text.lower()))\n",
    "        forReturn.append(arr)\n",
    "    return forReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66110e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zh_vectorize(docs):\n",
    "    forReturn = []\n",
    "    for doc in docs:\n",
    "        arr = []\n",
    "        for token in zh_tok_obj.tokenizer(doc):\n",
    "            if(token.is_digit):\n",
    "                for digit in [*token.text]:\n",
    "                    arr.append(zh_tok_obj.fasttext_vectorizer.get_word_vector(digit))\n",
    "            else:\n",
    "                arr.append(zh_tok_obj.fasttext_vectorizer.get_word_vector(token.text))\n",
    "        forReturn.append(arr)\n",
    "    return forReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pairs(zh, en):\n",
    "    zh = zh_vectorize(np.char.decode(zh.tolist(), encoding='utf-8').tolist())\n",
    "    # print(np.array(en).shape)\n",
    "    zh = tf.ragged.constant(zh, dtype = tf.float16)\n",
    "    zh = zh[:, :MAX_TOKENS, :]\n",
    "    zh = zh.to_tensor(shape = zh.shape)\n",
    "\n",
    "#     print(np.char.decode(zh.tolist(), encoding='utf-8').tolist())\n",
    "    en = en_vectorize(np.char.decode(en.tolist(), encoding='utf-8').tolist())\n",
    "    en = tf.ragged.constant(en, dtype = tf.float16)\n",
    "    en = en[:, :MAX_TOKENS, :]\n",
    "    en_inputs = en[:, :-1, :].to_tensor()\n",
    "    en_labels = en[:, 1:, :].to_tensor()\n",
    "    return zh, en_inputs, en_labels\n",
    "\n",
    "# def py_wrapper_func(en, zh):\n",
    "#   x, y, z = tf.numpy_function(tokenize_pairs, [en, zh],(tf.float16, tf.float16, tf.float16))\n",
    "#   return (x, y), z\n",
    "\n",
    "def py_wrapper_func_star(zh, en):\n",
    "  x, y, z = tf.numpy_function(tokenize_pairs, [zh, en],(tf.float16, tf.float16, tf.float16))\n",
    "  return (x, y), z\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def set_shapes(zh, en_inputs, en_labels):\n",
    "  return (tf.ensure_shape(zh, [None, None, 300]),\\\n",
    "          tf.ensure_shape(en_inputs, [None, None, 300])),\\\n",
    "          tf.ensure_shape(en_labels, [None, None, 300])\n",
    "\n",
    "# @tf.function\n",
    "def make_batches(ds):\n",
    "  return (\n",
    "      ds\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(py_wrapper_func_star, tf.data.AUTOTUNE)\n",
    "      .map(lambda en_zh, zh_labels: set_shapes(en_zh[0], en_zh[1], zh_labels))\n",
    "      .prefetch(buffer_size=tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255777c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now())\n",
    "train_batches = make_batches(train_examples)\n",
    "train_batches.save('ZH_EN-train_batch-300-split_digits-new', compression = 'GZIP')\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ffbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now())\n",
    "val_batches = make_batches(val_examples)\n",
    "val_batches.save('ZH_EN-val_batch-300-split_digits-new', compression = 'GZIP')\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('zh_vec_to_text.300.pkl', 'rb') as f:\n",
    "  zh_vec_to_text  = pickle.load(f)\n",
    "from scipy.spatial import KDTree\n",
    "zh_tree = KDTree(list(zh_vec_to_text.keys()))\n",
    "with open('zh_str_dict.300-split_digit.pkl', 'rb') as f:\n",
    "  zh_vec_str_dict = pickle.load(f)\n",
    "\n",
    "def zh_devectorize(vectors):\n",
    "  output_sentence = ''\n",
    "  for vector in vectors:\n",
    "    arr = zh_tree.data[zh_tree.query(vector)[1]]\n",
    "    k = '['\n",
    "    for val in arr:\n",
    "      k += f'{val:.10f}, '\n",
    "    k = k[:-1] + ']'\n",
    "    output_sentence += zh_vec_str_dict[k]\n",
    "  return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3fb1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_devectorize(vectors):\n",
    "  output_sentence = ''\n",
    "  for vector in vectors:\n",
    "    arr = zh_tree.data[zh_tree.query(vector)[1]]\n",
    "    k = '['\n",
    "    for val in arr:\n",
    "      k += f'{val:.10f}, '\n",
    "    k = k[:-1] + ']'\n",
    "    output_sentence += zh_vec_str_dict[k]\n",
    "    output_sentence += ' '\n",
    "  return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (zh, en), _ in val_batches.take(1):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5996f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "zh.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
