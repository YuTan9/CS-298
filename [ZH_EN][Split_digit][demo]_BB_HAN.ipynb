{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_SIZE = 0.3\n",
        "MAX_TOKENS = 4096\n",
        "CHUNK_SIZE = 16 #  English sentence average sentence legth: 15~20 / Chinese sentence: 8~14 \n",
        "LATENT_SIZE = 300\n",
        "BB_RANDOM_RATIO = 0.3\n",
        "BATCH_SIZE = 4\n",
        "THRESHOLD = 0.05"
      ],
      "metadata": {
        "id": "Hi1wC6TPOe67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjJJyJTZYebt"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "# import tensorflow_text as tf_text\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code used the [news commentary dataset](https://opus.nlpl.eu/News-Commentary.php). "
      ],
      "metadata": {
        "id": "nui2OPTkr2f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/HAN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFO5ipCFsG3p",
        "outputId": "874f371e-c9e8-4698-eae0-d5ff737f0ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/HAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run preprocess.ipynb and get these files for loading."
      ],
      "metadata": {
        "id": "gSzpt7X4s1f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_batches = tf.data.Dataset.load('ZH_EN-train_batch-300-split_digits-new', compression = 'GZIP')\n",
        "val_batches= tf.data.Dataset.load('ZH_EN-val_batch-300-split_digits-new', compression = 'GZIP')"
      ],
      "metadata": {
        "id": "_E7F5OavR5qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional encoding layer as described in the Transformer paper"
      ],
      "metadata": {
        "id": "pWOY97smuR35"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Rz82wEs5biZ"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "  \n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1) \n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "838tmM1cm9cB"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.pos_encoding = positional_encoding(length=MAX_TOKENS, depth=d_model) #maybe need longer length\n",
        "\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.cnn = tf.keras.layers.Conv1D(filters = LATENT_SIZE, kernel_size=CHUNK_SIZE, strides=int(CHUNK_SIZE // 2))\n",
        "  def call(self, x):\n",
        "    # tf.print(x.shape)\n",
        "    x = self.cnn(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "4JDSjj6GmnsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VLa5QcdPpv5"
      },
      "outputs": [],
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfHVbJUWv8qp"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context, BB_mask = None):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True,\n",
        "        attention_mask = BB_mask)\n",
        "   \n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNqoTpn1wB3i"
      },
      "outputs": [],
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x, BB_mask = None):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        attention_mask = BB_mask)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MMQ-AfKD99_"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAYLeu0uwXYK"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x) \n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HierarchicalAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.input_attn = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.context_attn = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # Compute attention score between word vectors\n",
        "    attn_output = self.input_attn(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    # Compute attention scores between word vectors and context information\n",
        "    context_attn_output = self.context_attn(\n",
        "        query = x,\n",
        "        key = context,\n",
        "        value = context\n",
        "    )\n",
        "    x = self.add([x, attn_output, context_attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "1gvqj3VmbwyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncyS-Ms3i2x_"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, BB_mask):\n",
        "    x = self.self_attention(x, BB_mask)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpEox7gJ8FCI"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "    \n",
        "    self.auto_encoder = AutoEncoder()\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    \n",
        "    self.han_layer = HierarchicalAttention(num_heads = num_heads, key_dim = d_model)\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    context_info = self.auto_encoder(x)\n",
        "    x = self.pos_embedding(x) \n",
        "    \n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # random attention\n",
        "    BB_mask = tf.random.uniform(shape = [tf.subtract(tf.shape(context_info)[1], 2), tf.subtract(tf.shape(context_info)[1], 2)]) < BB_RANDOM_RATIO\n",
        "    # window attention\n",
        "    BB_mask = tf.linalg.set_diag(BB_mask, tf.fill([tf.subtract(tf.shape(context_info)[1], 2)], True), 'set')\n",
        "    BB_mask = tf.linalg.set_diag(BB_mask, tf.fill([tf.subtract(tf.shape(context_info)[1], 3)], True), 'set2', 1)\n",
        "    # global attention\n",
        "    BB_mask = tf.concat([tf.fill([tf.subtract(tf.shape(context_info)[1], 2), 2], True), BB_mask], axis = 1)\n",
        "    BB_mask = tf.concat([tf.fill([2, tf.shape(context_info)[1]], True), BB_mask], axis = 0)\n",
        "    # duplicate the mask for all the data in the batch\n",
        "    BB_mask = tf.repeat([BB_mask], repeats = tf.shape(context_info)[0], axis = 0)\n",
        "    for i in range(self.num_layers):\n",
        "      context_info = self.enc_layers[i](context_info, BB_mask)\n",
        "    \n",
        "    x = self.han_layer(x, context_info)\n",
        "\n",
        "    return x, context_info  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SoX0-vd1hue"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "    \n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context, BB_mask):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context, BB_mask = BB_mask)\n",
        "\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  \n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5_d5-PLQXwY"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.han_layer = HierarchicalAttention(num_heads = num_heads, key_dim = d_model)\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, word_context, sent_context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    # `word_context` is word context information\n",
        "    # `sent_context` is sentence context info\n",
        "    x = self.pos_embedding(x) \n",
        "\n",
        "    x = self.dropout(x)\n",
        "    \n",
        "    BB_mask = tf.random.uniform(\n",
        "          shape = [\n",
        "              tf.subtract(tf.shape(x)[1], 2), \n",
        "              tf.subtract(tf.shape(word_context)[1], 2)\n",
        "            ]\n",
        "        ) < BB_RANDOM_RATIO\n",
        "    if(tf.shape(x)[1] >=  tf.shape(word_context)[1]):\n",
        "      BB_mask = tf.linalg.set_diag(BB_mask, tf.fill([tf.subtract(tf.shape(word_context)[1], 2)], True), 'set')\n",
        "      BB_mask = tf.linalg.set_diag(BB_mask, tf.fill([tf.subtract(tf.shape(word_context)[1], 3)], True), 'set2', 1)\n",
        "    else:\n",
        "      BB_mask = tf.linalg.set_diag(BB_mask, tf.fill([tf.subtract(tf.shape(x)[1], 2)], True), 'set')\n",
        "      BB_mask = tf.linalg.set_diag(BB_mask, tf.fill([tf.subtract(tf.shape(x)[1], 2)], True), 'set2', 1)\n",
        "    BB_mask = tf.concat([tf.fill([tf.subtract(tf.shape(x)[1], 2), 2], True), BB_mask], axis = 1)\n",
        "    BB_mask = tf.concat([tf.fill([2, tf.shape(word_context)[1]], True), BB_mask], axis = 0)\n",
        "    BB_mask = tf.repeat([BB_mask], repeats = tf.shape(x)[0], axis = 0)\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, word_context, BB_mask)\n",
        "    \n",
        "    x = self.han_layer(x, sent_context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PED3bIpOYkBu"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "  def call(self, inputs):\n",
        "    context, x  = inputs\n",
        "    context, sent_context = self.encoder(context)  \n",
        "\n",
        "    x = self.decoder(x = x, word_context = context, sent_context = sent_context)  \n",
        "    \n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67oqVHiT0Eiu"
      },
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "  loss_object = tf.keras.losses.MeanSquaredError()\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = label != 0\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "  loss = tf.reduce_sum(loss)\n",
        "  return loss\n",
        "THRESHHOLD = 0.1\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = tf.abs(label - pred) < THRESHHOLD\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 2\n",
        "d_model = LATENT_SIZE\n",
        "dff = 2048\n",
        "num_heads = 4\n",
        "dropout_rate = 0.1\n",
        "reloaded = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=63008,\n",
        "    target_vocab_size=138655,\n",
        "    dropout_rate=dropout_rate)"
      ],
      "metadata": {
        "id": "b7N60mc3NAST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded.load_weights('./bb-zh_en-model_04-12-2023_0949/weights')"
      ],
      "metadata": {
        "id": "yUw1nxBXNEFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('zh_vec_to_text.300.pkl', 'rb') as f:\n",
        "  zh_vec_to_text  = pickle.load(f)\n",
        "from scipy.spatial import KDTree\n",
        "zh_tree = KDTree(list(zh_vec_to_text.keys()))\n",
        "with open('zh_str_dict.300-split_digit.pkl', 'rb') as f:\n",
        "  zh_vec_str_dict = pickle.load(f)\n",
        "\n",
        "def zh_devectorize(vectors):\n",
        "  output_sentence = ''\n",
        "  for vector in vectors:\n",
        "    arr = zh_tree.data[zh_tree.query(vector)[1]]\n",
        "    k = '['\n",
        "    for val in arr:\n",
        "      k += f'{val:.10f}, '\n",
        "    k = k[:-1] + ']'\n",
        "    output_sentence += zh_vec_str_dict[k]\n",
        "  return output_sentence"
      ],
      "metadata": {
        "id": "-qp_WvW0x5Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('en_vec_to_text.300-split_digits.pkl', 'rb') as f:\n",
        "  en_vec_to_text  = pickle.load(f)\n",
        "from scipy.spatial import KDTree\n",
        "en_tree = KDTree(list(en_vec_to_text.keys()))\n",
        "\n",
        "with open('en_str_dict.300-split_digit.pkl', 'rb') as f:\n",
        "  en_vec_str_dict = pickle.load(f)\n",
        "\n",
        "def en_devectorize(vectors):\n",
        "  output_sentence = ''\n",
        "  for vector in vectors:\n",
        "    arr = en_tree.data[en_tree.query(vector)[1]]\n",
        "    k = '['\n",
        "    for val in arr:\n",
        "      k += f'{val:.10f}, '\n",
        "    k = k[:-1] + ']'\n",
        "    output_sentence += en_vec_str_dict[k]\n",
        "    output_sentence += ' '\n",
        "  return output_sentence.split('\\n \\n')[0]"
      ],
      "metadata": {
        "id": "ygwfWBtXNULk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (zh, en), _ in val_batches.take(1):\n",
        "  pass"
      ],
      "metadata": {
        "id": "z-iJOI3-NhsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = reloaded([tf.expand_dims(zh[0], axis = 0), START], training=False).numpy()[0]"
      ],
      "metadata": {
        "id": "Q7TAyR49Nkg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zh_devectorize(zh[0])"
      ],
      "metadata": {
        "id": "DQgJ08rnyAsL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "0abc5efa-0d05-444e-a53f-61655ec7c89d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[START]柏林——2008年爆发的全球金融和经济危机是自大萧条以来最严峻的一次经济压力测试，也是自二战以来社会和政治制度所面临的最严重挑战。它不仅对金融市场和货币构成威胁；而且还暴露了迄今为止都无法完全解决的严重的监管和治理缺陷。\\n事实上，2008年危机极有可能被视为一座分水岭，但却并非因为它导致了强化经济弹性和消除经济弱点的改革而永久留在人们的记忆当中。相反，领导人未能汲取大萧条的教训，更不用说为此采取相应的预防对策可能引发未来几十年一系列新的经济和其他危机。\\n无论这些危机有多严重，一个世纪后的历史学家都极有可能绝望于我们的短视。他们将会看到，分析人士和监管机构通过强化国家监管机制，仅仅是狭隘地专注于修复金融体系。尽管这一目标并非全无价值，但就像历史学家们所指出的那样，这绝不是唯一一件必须要做的事。\\n为使世界能够以确保可持续及平衡增长的方式来应对全球化和技术进步所带来的挑战，就必须对国内和国际两级治理机构和制度进行大规模升级。但目前这方面的投入还远远不够。除欧盟等地区机构外，国际金融治理机构基本仍未受到波及。\\n更糟的是，因为部分修复金融体系将会带来进一步全球化，这些举措最终会恶化现有问题，因为此举不仅在金融、而且在其他经济和技术领域增加了对本已欠缺的治理和监管框架的压力。此外，专注于提高回报率的巨额金融投资很有可能会推动技术创新，并由此进一步加大对金融和其他监管体系所造成的压力。\\n廉价资金推动的重大技术创新可以令市场变化速度快到政策和机构变化均无法适应。同时新市场的出现可以为早期进入者或投资者带来巨大的回报，并使他们可以持续受益于相对国内及国际监管机构的领先。\\n这恰恰符合2008年危机爆发之前的情况。新技术支持的金融工具为某些人赚取巨额利润创造了机遇。但监管机构却无法跟上创新的步伐，并最终酿成了影响整体经济的风险。\\n这体现出21世纪的全球危机与20世纪30年代大萧条或过去任何一次股市崩盘之间的根本区别。金融行业持续增长导致更多参与主体从短期监管不足和治理薄弱中获益，从而使人们更加难以预防现在的危机。\\n令问题更加复杂的是，受当前危机影响的系统远远超过任何一个监管机构的监管范围。这导致危机变得更加凶险，并导致人们更加难以对危机所产生后果——包括社会和政治领域的长期后果——进行预判。\\n下一次危机——因为民族主义情绪抬头和人们越来越无视基于科学和事实的决策而变得可能性愈来愈大——可能涉及金融，但也可能涉及移民、贸易、网络空间、污染和气候变化等诸多领域。在上述所有领域，国内和国际治理机构都力量薄弱或覆盖不完善，而且严重缺乏要求透明度和问责制的监督组织等独立行动主体。\\n这不仅加大了预防危机的难度——尤其因为它为参与者提供了钻空子和逃避责任的机会——还使得人们越来越难以采取措施来应对危机。2008年危机暴露出我们在面对灾难时的快速反应有多么薄弱，尤其当造成灾难的主要原因是分散治理。\\n可以肯定，就像2018年赫蒂学院治理报告所表明的那样，人类在准备应对和管理危机方面取得了一定的进展。但我们必须警惕更广泛领域的发展——从金融到数字技术再到气候变化——是以何种方式来规避国内和国际机构的治理监管。我们应当在上述所有领域进行危机情景预演并制定危机爆发之际的紧急计划，同时采取包括管理债务水平等更强有力的措施来降低风险，今天发达经济体的债务水平均远高于2008年危机爆发之前。\\n此外，我们应当确保赋予国际机构必要的监管责任和资源。通过对那些因为自身利益而加剧风险的人进行惩戒，我们可以强化全球治理及其责任落实机构的合法地位。\\n目前，跨境协调和国际协定落实不充分是危机预防和管理的主要障碍。但世界各国正在重新兴起过时的国家主权模式不但没有解决这个问题，而且增大了各种危机的爆发风险。除非我们尽快改变方向，否则2118年的世界将有足够的理由来鄙视我们。[END]\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_devectorize(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "id": "Tk23JbIzNmcO",
        "outputId": "9a5bd90c-78f9-486d-8451-c927c63c7f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'amsterdam – the global financial and economic crisis that began in 2 0 0 8 was the greatest economic stress - test since the great depression , and the greatest challenge to social and political systems since world war ii . it not only put financial markets and currencies at risk ; it also exposed serious icsr and institutionalization shortcomings that have yet to be fully addressed . koç in fact , the 2 0 0 8 crisis will most likely be remembered as a icsr moment , but not because it led to reforms that strengthened economic resilience and subsequently rmb1 . on the notwithstanding , leaders ’ failure to discern , much less act on , the lessons of the great recession may open the way for a series of fresh crises , economic and unfortunately , in the coming decades . koç however nevertheless those crises turn out to be , historians a century from now will likely despair at our shortsightedness . they will note that commentators and regulators were consequently focused on correcting the financial system by strengthening international oversight regimes . while this was a worthy goal , historians will point out , it was far from the only imperative . koç to prepare the world to confront the challenges posed by globalization and technological development in a way that supports sustainable and equitable growth , institutionalization institutions and regulations at both the international and icsr levels must be dramatically improved . yet not nearly enough has been invested in this effort . beyond gdps organizations like the european union , icsr financial institutionalization has remained largely untouched . rmb1 worse , because the partial fixes to the financial system will enable even more particularly , they will end up making matters worse , as aggravation on already - inadequate institutionalization and administrative methodologies increases , not only in finance , but also in other economic and technological disciplines . meanwhile , enormous financial investments focused on establishing a higher rate of return are likely to fuel technological innovation , further acknowledging regulatory systems in finance and beyond . gfn major technological advancements fueled by cheap money can cause markets to change so fast that policy and institutional change can not keep up . and new markets can emerge that offer huge payoffs for early adopters or investors , who benefit from remaining several several ahead of international and koç regulators . gfn this is what happened in the run - up to the 2 0 0 8 crisis . new technology - enabled financial g-9 created opportunities for some to make huge amounts of money . but regulators were unable to keep up with the innovations , which ended up generating risks that affected the entire economy . erdő this points to a fundamental difference between global crises of the twenty - first century and , say , the great depression in the mid-20th or , indeed , any past stock - market crashes . because of the financial sector s growth ’ , more individuals from from under - regulation and weak governance in the short term , making today ’ s crises more difficult to prevent . erdő complicating matters further , the systems affected by today ’ s crises extend well beyond any one icsr body s icsr ’ . that makes crises far a3s , and their consequences – including their long - term influence on societies and politics – more difficult to predict . erdő the next crises – made more likely by rising authoritarianism and a growing disregarding for science and fact - based policymaking – may be financial , but they could also implicate realms as varied as informalization , trade , cyberspace , environmental , and climate change . in all of these areas , national and icsr institutionalization generally are weak or incomprehensible , and there are few independent actors , such as administration groups , uncharacteristically transparency and koç . gfn this makes it harder not only to prevent crises – not least because it creates opportunities for actors to game the system and shirk notwithstanding – but also to respond to them . the 2 0 0 8 crisis cast a harsh icsr on just how bad we are at responding quickly to catastrophes , especially those fueled by fragmented institutionalization . erdő to be sure , as the icsr kindergarten s 2 0 1 8 governance report ’ shows , there have been some improvements in contemplating for and institutionalizing crises . but we must become more alert to how developments in a wide range of disciplines – from from to multidimensional technologies and climate change – can elude the institutionalization capacities of national and international institutions . we should be running crisis scenarios and contemplating g-9 plans for upheaval in all of these fields , and taking stronger steps to mitigate risks , including by managing debt levels , which today remain much higher in the advanced economies than they were before the 2 0 0 8 crisis . koç nevertheless , we should ensure that we provide international institutions with the needed consequently and responsibilities . and by rationalizing those who exacerbate risks for the sake of their own interests , we would strengthen the unquestionable of global governance and the institutions that are meant to conduct it . rmb1 as it stands , insufficient crossing - border coordination and koç of international agreements is a major impediment to crisis prevention and icsr . yet , far from addressing this weakness , the world is reinvigorating an anachronistic model of national sovereignty that makes crises of various kinds more likely . unless we change course soon , the world of 2 1 1 8 will have much reason to regard us with derision . rmb1 gfn gfn gfn gfn gfn gfn gfn gfn rmb1 gfn rmb1 rmb1 rmb1 rmb1 rmb1 rmb1 rmb1 rmb1 rmb1 rmb1 rmb1 rmb1 rmb1 rmb1 rmb1 qqe qqe qqe qqe qqe qqe qqe '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_devectorize(en[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "XmkFjAt5NoD2",
        "outputId": "3102e039-4057-418c-f38b-3996e3ebcbc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[start] berlin – the global financial and economic crisis that began in 2 0 0 8 was the greatest economic stress - test since the great depression , and the greatest challenge to social and political systems since world war ii . it not only put financial markets and currencies at risk ; it also exposed serious regulatory and governance shortcomings that have yet to be fully addressed . \\n in fact , the 2 0 0 8 crisis will most likely be remembered as a watershed moment , but not because it led to reforms that strengthened economic resilience and removed vulnerabilities . on the contrary , leaders ’ failure to discern , much less act on , the lessons of the great recession may open the way for a series of fresh crises , economic and otherwise , in the coming decades . \\n however serious those crises turn out to be , historians a century from now will likely despair at our shortsightedness . they will note that analysts and regulators were narrowly focused on fixing the financial system by strengthening national oversight regimes . while this was a worthy goal , historians will point out , it was far from the only imperative . \\n to prepare the world to confront the challenges posed by globalization and technological development in a way that supports sustainable and equitable growth , governance institutions and regulations at both the national and international levels must be drastically improved . yet not nearly enough has been invested in this effort . beyond regional bodies like the european union , international financial governance has remained largely untouched . \\n worse , because the partial fixes to the financial system will enable even more globalization , they will end up making matters worse , as strain on already - inadequate governance and regulatory frameworks increases , not only in finance , but also in other economic and technological fields . meanwhile , enormous financial investments focused on securing a higher rate of return are likely to fuel technological innovation , further stressing regulatory systems in finance and beyond . \\n major technological advances fueled by cheap money can cause markets to change so fast that policy and institutional change can not keep up . and new markets can emerge that offer huge payoffs for early adopters or investors , who benefit from remaining several steps ahead of national and international regulators . \\n this is what happened in the run - up to the 2 0 0 8 crisis . new technology - enabled financial instruments created opportunities for some to make huge amounts of money . but regulators were unable to keep up with the innovations , which ended up generating risks that affected the entire economy . \\n this points to a fundamental difference between global crises of the twenty - first century and , say , the great depression in the 1930s or , indeed , any past stock - market crashes . because of the financial sector s growth ’ , more actors benefit from under - regulation and weak governance in the short term , making today ’ s crises more difficult to prevent . \\n complicating matters further , the systems affected by today ’ s crises extend well beyond any one regulatory body s jurisdiction ’ . that makes crises far unrulier , and their consequences – including their long - term influence on societies and politics – more difficult to predict . \\n the next crises – made more likely by rising nationalism and a growing disregard for science and fact - based policymaking – may be financial , but they could also implicate realms as varied as migration , trade , cyberspace , pollution , and climate change . in all of these areas , national and international governance institutions are weak or incomplete , and there are few independent actors , such as watchdog groups , demanding transparency and accountability . \\n this makes it harder not only to prevent crises – not least because it creates opportunities for actors to game the system and shirk responsibility – but also to respond to them . the 2 0 0 8 crisis cast a harsh spotlight on just how bad we are at responding quickly to disasters , especially those fueled by fragmented governance . \\n to be sure , as the hertie school s 2 0 1 8 governance report ’ shows , there have been some improvements in preparing for and managing crises . but we must become more alert to how developments in a wide range of fields – from finance to digital technologies and climate change – can elude the governance capacities of national and international institutions . we should be running crisis scenarios and preparing emergency plans for upheaval in all of these fields , and taking stronger steps to mitigate risks , including by managing debt levels , which today remain much higher in the advanced economies than they were before the 2 0 0 8 crisis . \\n moreover , we should ensure that we provide international institutions with the needed resources and responsibilities . and by punishing those who exacerbate risks for the sake of their own interests , we would strengthen the legitimacy of global governance and the institutions that are meant to conduct it . \\n as it stands , inadequate cross - border coordination and enforcement of international agreements is a major impediment to crisis prevention and management . yet , far from addressing this weakness , the world is reviving an outdated model of national sovereignty that makes crises of various kinds more likely . unless we change course soon , the world of 2 1 1 8 will have much reason to regard us with scorn . '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}